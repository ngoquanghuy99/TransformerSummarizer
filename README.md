## Abstractive summarization using Generative Pretrained Transformer (GPT-2)

This is my Trax implementation of GPT-2 (Transformer Decoder) for one of the Natural Language Generation task, Abstractive summarization.

Paper: [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf).

Library: [Trax](https://github.com/google/trax) - Deep Learning Library in [JAX](https://github.com/google/jax) actively used and maintained in the [Google Brain](https://research.google.com/teams/brain/) team.

Dataset: https://www.kaggle.com/shashichander009/inshorts-news-data
